{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zRftAQl_Zwr"
   },
   "source": [
    "# Curiosity-Based Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5jNwie85PNO"
   },
   "source": [
    "## Imports and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2692,
     "status": "ok",
     "timestamp": 1675110716395,
     "user": {
      "displayName": "Giulio Vaccari",
      "userId": "07259365636674343883"
     },
     "user_tz": -60
    },
    "id": "wVkjEXEE401T",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# External Libraries\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Visualization of gameplay inside notebook\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8851,
     "status": "ok",
     "timestamp": 1675110725234,
     "user": {
      "displayName": "Giulio Vaccari",
      "userId": "07259365636674343883"
     },
     "user_tz": -60
    },
    "id": "pZRhZp2ik5V0"
   },
   "outputs": [],
   "source": [
    "# Load Project modules\n",
    "\n",
    "from envs import make_env\n",
    "from agents import build_agent\n",
    "from prioritizers.priority_combinator import PriorityCombinator\n",
    "from prioritizers.rnd_prioritizer import RNDPrioritizer\n",
    "from prioritizers.aesh_prioritizer import AESHPrioritizer\n",
    "from prioritizers.icm_prioritizer import ICMPrioritizer\n",
    "from prioritizers.td_prioritizer import TDPrioritizer\n",
    "from training import train\n",
    "from utils.eval_scores import *\n",
    "from utils.plot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 936,
     "status": "ok",
     "timestamp": 1675110726133,
     "user": {
      "displayName": "Giulio Vaccari",
      "userId": "07259365636674343883"
     },
     "user_tz": -60
    },
    "id": "Ns7Of6tG47d7",
    "outputId": "8e28c792-93c9-4959-b4c2-c3997c54cd56"
   },
   "outputs": [],
   "source": [
    "# CHECKING TENSORFLOW SUPPORT FOR NVIDIA GPU\n",
    "\n",
    "print(\"Checking available GPUs...\")\n",
    "gpu_available = len(tf.config.list_physical_devices('GPU')) > 0\n",
    "if gpu_available:\n",
    "    print(\"GPU available\")\n",
    "else:\n",
    "    print(\"GPU not available\")\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1675110726134,
     "user": {
      "displayName": "Giulio Vaccari",
      "userId": "07259365636674343883"
     },
     "user_tz": -60
    },
    "id": "1RDBtrqnuI6k"
   },
   "outputs": [],
   "source": [
    "# Definition of Gym envs params\n",
    "\n",
    "envs_params = {\n",
    "    #### BASIC GYM ENVs\n",
    "    \"CartPole-v1\" : {\n",
    "        \"max_timesteps\":200,\n",
    "        \"conv\": False\n",
    "    },\n",
    "    \"MountainCar-v0\" : {\n",
    "        \"max_timesteps\":200,\n",
    "        \"conv\": False\n",
    "    },\n",
    "    \"Acrobot-v1\" : {\n",
    "        \"max_timesteps\":200,\n",
    "        \"conv\": False\n",
    "    },\n",
    "    #### Require:\n",
    "    #### pip install Box2D\n",
    "    \"LunarLander-v2\" : {\n",
    "        \"max_timesteps\":1000,\n",
    "        \"conv\": False\n",
    "    },\n",
    "    ##### ATARI GAMES\n",
    "    \"ALE/WizardOfWor-v5\" : {\n",
    "        \"max_timesteps\":5000,\n",
    "        \"conv\": True\n",
    "    },\n",
    "    \"ALE/Freeway-v5\" : {\n",
    "        \"max_timesteps\":5000,\n",
    "        \"conv\": True\n",
    "    },\n",
    "    \"ALE/Gravitar-v5\" : {\n",
    "        \"max_timesteps\":5000,\n",
    "        \"conv\": True\n",
    "    },\n",
    "    \"ALE/MontezumaRevenge-v5\" : {\n",
    "        \"max_timesteps\":2000,\n",
    "        \"conv\":True\n",
    "    },\n",
    "    \"ALE/Venture-v5\" : {\n",
    "        \"max_timesteps\":2000,\n",
    "        \"conv\":True\n",
    "    },\n",
    "    \"ALE/Breakout-v5\" : {\n",
    "        \"max_timesteps\":2000,\n",
    "        \"conv\":True\n",
    "    },\n",
    "    \"ALE/Pong-v5\" : {\n",
    "        \"max_timesteps\":2000,\n",
    "        \"conv\":True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1675110726515,
     "user": {
      "displayName": "Giulio Vaccari",
      "userId": "07259365636674343883"
     },
     "user_tz": -60
    },
    "id": "brwX1wXev0Ov",
    "outputId": "8bd373c3-a73c-4982-ca5c-5e063e5a76b2"
   },
   "outputs": [],
   "source": [
    "# Define testing environment\n",
    "\n",
    "env_name = \"MountainCar-v0\"\n",
    "\n",
    "env_conv = envs_params[env_name][\"conv\"]\n",
    "use_max_timesteps = False\n",
    "clip_reward_training = False\n",
    "\n",
    "if use_max_timesteps:\n",
    "    max_episode_steps = envs_params[env_name][\"max_timesteps\"]\n",
    "else:\n",
    "    max_episode_steps = None #1e7\n",
    "\n",
    "env = make_env(env_name, env_conv, clip_reward=clip_reward_training, max_episode_steps=max_episode_steps)\n",
    "eval_env = make_env(env_name, env_conv, clip_reward=False, max_episode_steps=max_episode_steps)\n",
    "\n",
    "print(\"env.action_space: {}\".format(env.action_space))\n",
    "print(\"env.observation_space: {}\".format(env.observation_space))\n",
    "print(\"env.observation_space.shape: {}\".format(env.observation_space.shape))\n",
    "print(\"env.action_space.n: {}\".format(env.action_space.n))\n",
    "\n",
    "input_shape = env.observation_space.shape # Shape of a game state / observation\n",
    "n_actions = env.action_space.n # Number of possible actions available each turn for the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1675110726516,
     "user": {
      "displayName": "Giulio Vaccari",
      "userId": "07259365636674343883"
     },
     "user_tz": -60
    },
    "id": "fRqRADy4k5V2"
   },
   "outputs": [],
   "source": [
    "# if Atari, Show a frame of the game\n",
    "if env_conv:\n",
    "    debug_env = make_env(env_name, env_conv, max_episode_steps=max_episode_steps, render_mode=\"rgb_array\")\n",
    "    debug_obs = debug_env.reset(seed=1)[0]\n",
    "    for i in range(100):\n",
    "        debug_obs, _, _, _, _ = debug_env.step(1)\n",
    "    plt.imshow(debug_obs[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wkg8qDhn57v2"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1675110726820,
     "user": {
      "displayName": "Giulio Vaccari",
      "userId": "07259365636674343883"
     },
     "user_tz": -60
    },
    "id": "bzOHofKHIciM"
   },
   "outputs": [],
   "source": [
    "# Training session and Hyper-parameters definition\n",
    "\n",
    "# List of models to test (it is possible to split the models in order to perform the testing in multiple sessions)\n",
    "models_to_test = [\"AESH\", \"ICM\", \"DIST\", \"TD\", \"UNIFORM\"]\n",
    "\n",
    "\n",
    "# If the actual configuration has to be loaded from an external confi file.\n",
    "# If False, it will use the following parameters.\n",
    "load_config_from_json = True\n",
    "config_file_name = \"Generic-DQN-400K.json\"\n",
    "\n",
    "# ATTENTION: These params are used only if load_config_from_json == False\n",
    "agent_params = {\n",
    "    \"DQN\": {\n",
    "        \"lr\": 1e-3,\n",
    "        \"grad_clipping\":None,\n",
    "        \"gamma\":0.99\n",
    "    },\n",
    "    \"SAC\": {\n",
    "        \"lr_critic\":1e-3,\n",
    "        \"lr_actor\":1e-3,\n",
    "        \"lr_alpha\":1e-3,\n",
    "        \"gamma\":0.99\n",
    "    }\n",
    "}\n",
    "buffer_params = {\n",
    "    \"buffer_size\": 300_000,\n",
    "    \"prioritized_replay_alpha\": 0.8,\n",
    "    \"prioritized_replay_beta0\": 0.2,\n",
    "    \"exploration_fraction\": 0.2,\n",
    "    \"exploration_final_eps\": 0.001\n",
    "}\n",
    "training_params = {\n",
    "    \"agent_train_freq\": 4,\n",
    "    \"total_timesteps\": 300_000,\n",
    "    \"learning_starts\": 1000,\n",
    "    \"batch_size\": 64,\n",
    "    \"eval_freq\": 3000, # in timesteps\n",
    "    \"k_rollouts\": 10,\n",
    "    \"buffer_prs_plot_freq\": 30000\n",
    "}\n",
    "priority_combinator_params = {\n",
    "    \"w_ic\": 1,\n",
    "    \"w_td\": 0,\n",
    "    \"w_rw\": 0,\n",
    "}\n",
    "rnd_prioritizer_params = {\n",
    "    \"learning_rate\": 1e-3\n",
    "}\n",
    "aesh_prioritizer_params = {\n",
    "    \"k\": 64 if env_conv else 32, # Length of the binary code\n",
    "    \"encoder_output_size\": 256 if env_conv else 32,\n",
    "    \"lambda_regularizer\": 10,\n",
    "    \"beta\": 1, # p = beta/sqrt(N(phi(s)))\n",
    "    \"learning_rate\": 1e-3\n",
    "}\n",
    "icm_prioritizer_params = {\n",
    "    \"embedding_size\": 288 if env_conv else 32, #288,\n",
    "    \"beta\": 0.2,\n",
    "    \"learning_rate\": 1e-3\n",
    "}\n",
    "\n",
    "hyper_params_dict = {\n",
    "    \"selected_agent\": \"DQN\", #Choose between DQN or SAC,\n",
    "    \"use_priority_combinator\": True, # True to use the priority combinator to normalize and combine the priorities\n",
    "    \"agent_params\": agent_params,\n",
    "    \"buffer_params\": buffer_params,\n",
    "    \"training_params\": training_params,\n",
    "    \"priority_combinator_params\": priority_combinator_params,\n",
    "    \"prioritizers_params\": {\n",
    "        \"DIST\": rnd_prioritizer_params,\n",
    "        \"AESH\": aesh_prioritizer_params,\n",
    "        \"ICM\": icm_prioritizer_params\n",
    "    },\n",
    "    \"episodes_print_freq\": 50,\n",
    "    \"smoothing_window_size\": 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1675110727077,
     "user": {
      "displayName": "Giulio Vaccari",
      "userId": "07259365636674343883"
     },
     "user_tz": -60
    },
    "id": "HAaO564Irm79"
   },
   "outputs": [],
   "source": [
    "# Write configuration file\n",
    "'''\n",
    "conifg_file_path = os.path.join(\"configs\", \"MountainCarConfig.json\")\n",
    "with open(conifg_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(hyper_params_dict, f, ensure_ascii=False, indent=4)\n",
    "'''\n",
    "\n",
    "# Load configuration file\n",
    "if load_config_from_json:\n",
    "    # Read a configuration file\n",
    "    conifg_file_path = os.path.join(\"configs\", config_file_name)\n",
    "    with open(conifg_file_path, 'r') as file:\n",
    "        hyper_params_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1675110727078,
     "user": {
      "displayName": "Giulio Vaccari",
      "userId": "07259365636674343883"
     },
     "user_tz": -60
    },
    "id": "LINI-b7iIFKW"
   },
   "outputs": [],
   "source": [
    "# Create plots directory if it not exists\n",
    "if not os.path.exists(\"plots\"):\n",
    "    os.makedirs(\"plots\")\n",
    "\n",
    "dir_env_plots = os.path.join(\"plots\", format_plots_path_name(env_name=env_name, suffix=hyper_params_dict[\"selected_agent\"]))\n",
    "if not os.path.exists(dir_env_plots):\n",
    "    os.makedirs(dir_env_plots)\n",
    "\n",
    "# Create dfs directory if it not exists\n",
    "if not os.path.exists(\"dfs\"):\n",
    "    os.makedirs(\"dfs\")\n",
    "\n",
    "dir_env_dfs = os.path.join(\"dfs\", format_plots_path_name(env_name=env_name, suffix=hyper_params_dict[\"selected_agent\"]))\n",
    "if not os.path.exists(dir_env_dfs):\n",
    "    os.makedirs(dir_env_dfs)\n",
    "\n",
    "# Create models directory if it not exists\n",
    "if not os.path.exists(\"models_checkpoints\"):\n",
    "    os.makedirs(\"models_checkpoints\")\n",
    "\n",
    "dir_env_checkpoints = os.path.join(\"models_checkpoints\", format_plots_path_name(env_name=env_name, suffix=hyper_params_dict[\"selected_agent\"]))\n",
    "if not os.path.exists(dir_env_checkpoints):\n",
    "    os.makedirs(dir_env_checkpoints)\n",
    "\n",
    "# Create configs directory if it not exists\n",
    "if not os.path.exists(\"configs\"):\n",
    "    os.makedirs(\"configs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed):    \n",
    "    # Python Random Library\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Tensorflow\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1675110727080,
     "user": {
      "displayName": "Giulio Vaccari",
      "userId": "07259365636674343883"
     },
     "user_tz": -60
    },
    "id": "2NcW9A5YRZ8q"
   },
   "outputs": [],
   "source": [
    "def plot_rewards_together(models_to_plot, selected_agent):\n",
    "    # Load previously saved eval returns of models\n",
    "    # It's usefull if the training of the different models is performed in different sessions\n",
    "    eval_returns_list = []\n",
    "    for model in models_to_plot:\n",
    "        eval_returns = pd.read_csv(os.path.join(dir_env_dfs, \"eval_returns_\"+selected_agent+\"_\"+model+\".csv\"), converters={\"y\": literal_eval})\n",
    "        eval_returns_list.append(eval_returns)\n",
    "    # Plot together all the reward curves\n",
    "    print_results_from_dataframe_ci(eval_returns_list,\n",
    "                                    models_to_plot,\n",
    "                                    file_name=format_plots_path_name(dir=dir_env_plots,\n",
    "                                                                     env_name=env_name,\n",
    "                                                                     suffix=\"reward_all\"+\"_\"+selected_agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1675110727421,
     "user": {
      "displayName": "Giulio Vaccari",
      "userId": "07259365636674343883"
     },
     "user_tz": -60
    },
    "id": "hCV-zT4GRZ8q"
   },
   "outputs": [],
   "source": [
    "def combine_rollouts_dfs(dfs_list, x_name=\"x\", y_name=\"y\"):\n",
    "    # Starting from the dfs with the single-run evaluations, create a single dataframe with all the \n",
    "    # evaluations in a single column as a list.\n",
    "    combined_df = pd.DataFrame(data={x_name:dfs_list[0][x_name]})\n",
    "    indices_list = []\n",
    "    for k, df_k in enumerate(dfs_list):\n",
    "        indices_list.append(k)\n",
    "        combined_df[k] = df_k[y_name]\n",
    "    combined_df[y_name] = combined_df[indices_list].values.tolist()\n",
    "    combined_df.drop(columns=indices_list, inplace=True)\n",
    "    return combined_df\n",
    "\n",
    "def train_models(models_to_test,\n",
    "                 hyper_params_dict,\n",
    "                 env,\n",
    "                 eval_env,\n",
    "                 train_seeds,\n",
    "                 eval_seeds,\n",
    "                 env_conv,\n",
    "                 env_name,\n",
    "                 dir_env_plots,\n",
    "                 dir_env_dfs,\n",
    "                 dir_env_checkpoints):\n",
    "    input_shape = env.observation_space.shape # Shape of a game state / observation\n",
    "    n_actions = env.action_space.n # Number of possible actions available each turn for the agent\n",
    "    selected_agent = hyper_params_dict[\"selected_agent\"]\n",
    "    k_rollouts = hyper_params_dict[\"training_params\"][\"k_rollouts\"]\n",
    "    use_priority_combinator = hyper_params_dict[\"use_priority_combinator\"]\n",
    "    episodes_print_freq = hyper_params_dict[\"episodes_print_freq\"]\n",
    "    smoothing_window_size = hyper_params_dict[\"smoothing_window_size\"]\n",
    "    for model in models_to_test:\n",
    "        # Pretty Printing\n",
    "        print(\"###################################################\")\n",
    "        print(\"TRAINING \"+model+\" MODEL:\")\n",
    "        \n",
    "        eval_dfs_list = []\n",
    "        pr_hist_list = []\n",
    "        buffer_prs_list = []\n",
    "        start_time = time.time()\n",
    "        for rollout_n in range(k_rollouts):\n",
    "            print(\"ROLLOUT N. \"+str(rollout_n+1)+\":\")\n",
    "            \n",
    "            # Set global seed for the model\n",
    "            set_global_seed(train_seeds[rollout_n])\n",
    "            \n",
    "            # Does the model use a prioritized replay?\n",
    "            prioritized_replay = model != \"UNIFORM\"\n",
    "\n",
    "            # Build agent\n",
    "            agent = build_agent(selected_agent, env, env_conv, hyper_params_dict[\"agent_params\"][selected_agent])\n",
    "\n",
    "            # Build priority combinator\n",
    "            if use_priority_combinator and prioritized_replay:\n",
    "                priority_combinator = PriorityCombinator(**hyper_params_dict[\"priority_combinator_params\"])\n",
    "            else:\n",
    "                priority_combinator = None\n",
    "\n",
    "            # Build prioritizer\n",
    "            if model == \"DIST\":\n",
    "                prioritizer = RNDPrioritizer(input_shape, n_actions, conv=env_conv,\n",
    "                                             params_dict=hyper_params_dict[\"prioritizers_params\"][model],\n",
    "                                             priority_combinator=priority_combinator)\n",
    "            elif model == \"ICM\":\n",
    "                prioritizer = ICMPrioritizer(input_shape, n_actions, conv=env_conv,\n",
    "                                             params_dict=hyper_params_dict[\"prioritizers_params\"][model],\n",
    "                                             priority_combinator=priority_combinator)\n",
    "            elif model == \"AESH\":\n",
    "                prioritizer = AESHPrioritizer(input_shape, n_actions, conv=env_conv,\n",
    "                                              params_dict=hyper_params_dict[\"prioritizers_params\"][model],\n",
    "                                              priority_combinator=priority_combinator)\n",
    "            elif model == \"TD\":\n",
    "                prioritizer = TDPrioritizer()\n",
    "            elif model == \"UNIFORM\":\n",
    "                prioritizer = None\n",
    "            else:\n",
    "                raise Exception(\"Incorrect model specification\")\n",
    "\n",
    "            # Perform training\n",
    "            env.reset(seed=train_seeds[rollout_n])\n",
    "            eval_env.reset(seed=eval_seeds[rollout_n])\n",
    "            _, evaluations, ep_returns, pr_hist, buffer_prs = train(agent, env, eval_env, prioritizer,\n",
    "                                                                     episodes_print_freq=episodes_print_freq,\n",
    "                                                                     buffer_params=hyper_params_dict[\"buffer_params\"],\n",
    "                                                                     training_params=hyper_params_dict[\"training_params\"],\n",
    "                                                                     prioritized_replay=prioritized_replay)\n",
    "            eval_dfs_list.append(evaluations)\n",
    "            pr_hist_list.append(pr_hist)\n",
    "            buffer_prs_list.append(buffer_prs)\n",
    "            print()\n",
    "            print(\"***************************************************\")\n",
    "        print(f\"Training time: {(time.time()-start_time)/60} minutes\")\n",
    "        \n",
    "        # Combine rollouts dfs in a single dataframe\n",
    "        eval_returns_comb = combine_rollouts_dfs(eval_dfs_list, x_name=\"x\", y_name=\"y\")\n",
    "        \n",
    "        # Save training data\n",
    "        # Episodes rewards\n",
    "        print_ep_rewards_from_dataframe(ep_returns, model,\n",
    "                                        file_name=format_plots_path_name(dir=dir_env_plots,\n",
    "                                                                         env_name=env_name,\n",
    "                                                                         suffix=\"ep_\"+selected_agent+\"_\"+model))\n",
    "        # Step reward\n",
    "        print_results_from_dataframe_ci([eval_returns_comb],\n",
    "                                        [model],\n",
    "                                        file_name=format_plots_path_name(dir=dir_env_plots,\n",
    "                                                                         env_name=env_name,\n",
    "                                                                         suffix=\"reward_\"+selected_agent+\"_\"+model),\n",
    "                                        ylabel=\"Reward\")\n",
    "        # Save also as a csv\n",
    "        eval_returns_comb.to_csv(os.path.join(dir_env_dfs, \"eval_returns_\"+selected_agent+\"_\"+model+\".csv\"), index=False)\n",
    "        \n",
    "        if prioritizer is not None:\n",
    "            # Combine rollouts dfs in a single dataframe\n",
    "            pr_hist_comb = combine_rollouts_dfs(pr_hist_list, x_name=\"x\", y_name=\"y\")\n",
    "            buffer_prs_comb = combine_rollouts_dfs(buffer_prs_list, x_name=\"tms\", y_name=\"prs\")\n",
    "            \n",
    "            # Priority history\n",
    "            print_results_from_dataframe_ci([pr_hist_comb],\n",
    "                                            [model],\n",
    "                                            file_name=format_plots_path_name(dir=dir_env_plots,\n",
    "                                                                             env_name=env_name,\n",
    "                                                                             suffix=\"pr_hist_\"+selected_agent+\"_\"+model),\n",
    "                                            ylabel=\"Max Priority\")\n",
    "            \n",
    "            # Priorities stored in the buffer during training\n",
    "            print_buffer_priorities_from_dataframe(buffer_prs_comb,\n",
    "                                                   file_name=format_plots_path_name(dir=dir_env_plots,\n",
    "                                                                                    env_name=env_name,\n",
    "                                                                                    suffix=\"buffer_prs_\"+selected_agent+\"_\"+model))\n",
    "            \n",
    "            # Save priority history and priorities stored in the buffer as csv\n",
    "            pr_hist_comb.to_csv(os.path.join(dir_env_dfs, \"pr_hist_\"+selected_agent+\"_\"+model+\".csv\"), index=False)\n",
    "            buffer_prs_comb.to_csv(os.path.join(dir_env_dfs, \"buffer_prs_\"+selected_agent+\"_\"+model+\".csv\"), index=False)\n",
    "            \n",
    "        # Save trained agent\n",
    "        agent.save_model(dir_env_checkpoints, (selected_agent+\"_\"+model))\n",
    "        \n",
    "        # Pretty Printing\n",
    "        print()\n",
    "        print()\n",
    "    \n",
    "    # Plot together all the reward curves\n",
    "    plot_rewards_together(models_to_test, selected_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for producing all the other seeds\n",
    "set_global_seed(42)\n",
    "\n",
    "if False:\n",
    "    # Generate Gym random seeds\n",
    "    train_seeds = []\n",
    "    eval_seeds = []\n",
    "    k_rollouts = hyper_params_dict[\"training_params\"][\"k_rollouts\"]\n",
    "    max_seed_n = 1000\n",
    "    for i in range(k_rollouts):\n",
    "        train_seeds.append(np.random.randint(max_seed_n))\n",
    "        eval_seeds.append(np.random.randint(max_seed_n))\n",
    "    print(\"GYM TRAINING SEEDS: \"+str(train_seeds))\n",
    "    print(\"GYM EVALUATION SEEDS: \"+str(eval_seeds))\n",
    "    \n",
    "else:\n",
    "    train_seeds = [102, 860, 106, 700, 614, 466, 330, 87, 99, 663]\n",
    "    eval_seeds = [435, 270, 71, 20, 121, 214, 458, 372, 871, 130]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FHJFH4K6RZ8q",
    "outputId": "ccc3c159-46c2-4a09-ac23-51af48d48de1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_models(models_to_test,\n",
    "             hyper_params_dict,\n",
    "             env,\n",
    "             eval_env,\n",
    "             train_seeds,\n",
    "             eval_seeds,\n",
    "             env_conv,\n",
    "             env_name,\n",
    "             dir_env_plots,\n",
    "             dir_env_dfs,\n",
    "             dir_env_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lR7L9FfORZ8r"
   },
   "outputs": [],
   "source": [
    "# Plot together the reward curves of all 5 models\n",
    "plot_rewards_together([\"AESH\", \"ICM\", \"DIST\", \"TD\", \"UNIFORM\"], hyper_params_dict[\"selected_agent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average final scores\n",
    "eval_scores = compute_eval_scores_tms(dir_env_dfs, [\"AESH\", \"ICM\", \"DIST\", \"TD\", \"UNIFORM\"],\n",
    "                                      hyper_params_dict[\"selected_agent\"])\n",
    "\n",
    "# Print final scores\n",
    "print_eval_scores(eval_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT COLORED REWARDS AND PRIORITIES (eventually overrides previous mono-colored plots)\n",
    "\n",
    "color_index = 0\n",
    "for model in [\"AESH\", \"ICM\", \"DIST\", \"TD\", \"UNIFORM\"]:\n",
    "    color = \"C\" + str(color_index)\n",
    "    # PLOT REWARD\n",
    "    eval_returns = pd.read_csv(os.path.join(dir_env_dfs, \"eval_returns_\"+selected_agent+\"_\"+model+\".csv\"), converters={\"y\": literal_eval})\n",
    "    print_results_from_dataframe_ci([eval_returns],\n",
    "                                    [model],\n",
    "                                    file_name=format_plots_path_name(dir=dir_env_plots,\n",
    "                                                                     env_name=env_name,\n",
    "                                                                     suffix=\"reward_\"+selected_agent+\"_\"+model),\n",
    "                                    color=color)\n",
    "\n",
    "    # PLOT PRIORITIES\n",
    "    pr_hist = pd.read_csv(os.path.join(dir_env_dfs, \"pr_hist_\"+selected_agent+\"_\"+model+\".csv\"), converters={\"y\": literal_eval})\n",
    "    print_results_from_dataframe_ci([pr_hist],\n",
    "                                    [model],\n",
    "                                    file_name=format_plots_path_name(dir=dir_env_plots,\n",
    "                                                                     env_name=env_name,\n",
    "                                                                     suffix=\"pr_hist_\"+selected_agent+\"_\"+model),\n",
    "                                    ylabel=\"Max Priority\",\n",
    "                                    color=color)\n",
    "    \n",
    "    color_index = color_index + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGyZbAZck5V7"
   },
   "source": [
    "## Play a Game with the Curiosity-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqAdvZ6pMyrI"
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "selected_agent = hyper_params_dict[\"selected_agent\"]\n",
    "agent = build_agent(selected_agent, env, env_conv, hyper_params_dict[\"agent_params\"][selected_agent])\n",
    "agent.load_model(dir_env_checkpoints, selected_agent+\"_\"+\"ICM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qq__tWmik5V7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    model = agent\n",
    "    play_env = make_env(env_name, env_conv, clip_reward=False, max_episode_steps=None, render_mode=\"rgb_array\")\n",
    "    for episode in range(1): # Play one game\n",
    "        obs = play_env.reset()[0]\n",
    "        total_rew = 0\n",
    "        t = 0\n",
    "        while True:\n",
    "            #env.render()\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(play_env.render())\n",
    "            plt.show()\n",
    "            \n",
    "            obs = np.expand_dims(np.array(obs), axis=0)\n",
    "            obs_tf = tf.constant(obs)\n",
    "            action = model.step(obs_tf, stochastic=False)\n",
    "            \n",
    "            obs, rew, done, truncated, _  = play_env.step(int(action))\n",
    "            total_rew = total_rew + rew\n",
    "            t = t + 1\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        print(\"Score: {}\".format(total_rew))\n",
    "    play_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPisuOFYk5V7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GS2avd7vk5V8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRsgZrJrk5V8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
